# model/generator.py
import torch
from miditok import REMI
from miditoolkit import MidiFile
from model.music_transformer import MusicTransformer
import random, os

# ==============================
# ğŸµ Tokenizer ì´ˆê¸°í™”
# ==============================
tokenizer = REMI()

# vocab ì„¤ì • (BPEê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ vocab ì‚¬ìš©)
if hasattr(tokenizer, "vocab_bpe") and tokenizer.vocab_bpe is not None:
    vocab_size = len(tokenizer.vocab_bpe)
else:
    vocab_size = len(tokenizer.vocab)

# ==============================
# ğŸ¶ ëª¨ë¸ ì´ˆê¸°í™”
# ==============================
model = MusicTransformer(vocab_size=vocab_size)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


# ==============================
# ğŸ¼ ìŒì•… ìƒì„± í•¨ìˆ˜
# ==============================
def generate_music(genre="pop", mood="happy", duration=10):
    model.eval()

    # 1ï¸âƒ£ ì‹œì‘ í† í° ì„¤ì •
    start_token = torch.tensor([[0]], device=device)  # ë‹¨ìˆœí•œ ì‹œì‘ í† í°
    seq = start_token
    generated = [start_token.item()]

    # 2ï¸âƒ£ í† í° ìƒì„± (ìƒ˜í”Œ ì˜ˆì‹œ: 50ê°œ)
    for _ in range(50):
        with torch.no_grad():
            logits = model(seq)
            next_token = torch.argmax(logits[:, -1, :], dim=-1)
        generated.append(next_token.item())
        seq = torch.cat([seq, next_token.unsqueeze(0)], dim=1)

    # 3ï¸âƒ£ MIDI ë³€í™˜
    vocab_len = vocab_size
    decoded_tokens = [int(tok) % vocab_len for tok in generated]

    try:
        midi = tokenizer.tokens_to_midi(decoded_tokens)
    except Exception as e:
        print(f"âŒ MIDI ë³€í™˜ ì‹¤íŒ¨: {e}")
        midi = MidiFile()

    # 4ï¸âƒ£ íŒŒì¼ ì €ì¥
    os.makedirs("outputs", exist_ok=True)
    out_path = os.path.join("outputs", f"{genre}_{mood}.mid")
    midi.dump(out_path)

    print(f"âœ… ìƒì„± ì™„ë£Œ: {out_path}")
    return out_path
